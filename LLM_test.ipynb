{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from POP import PromptFunction\n",
    "#from POP.utils import load_prompt, get_text_snapshot\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"a021e45e-0446-49d0-855a-a15021a17964\", \"model\": \"sonar\", \"created\": 1740437547, \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 130, \"total_tokens\": 140}, \"citations\": [\"https://www.dw.com/en/earth-is-warming-so-why-is-it-so-cold/a-71661055\", \"https://www.climate.gov/news-features/blogs/polar-vortex/cold-air-its-probably-not-polar-vortex\", \"https://www.nbcwashington.com/weather/science-4-everyone-how-do-bugs-survive-during-the-cold-winter-months/3850831/\"], \"object\": \"chat.completion\", \"choices\": [{\"index\": 0, \"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Winter cold is primarily due to Earth's tilt away from the Sun during this season, reducing the amount of solar energy received by the Northern Hemisphere. Additionally, cold air masses from polar regions can move towards lower latitudes, bringing frigid temperatures. In recent years, phenomena like the **polar vortex** have been linked to extreme cold snaps. The polar vortex is a band of winds that usually keeps cold air contained over the poles. However, when it weakens or becomes more unstable due to climate change, it can allow cold air to escape and move towards more temperate regions, leading to sudden and intense cold snaps[1][2].\"}, \"delta\": {\"role\": \"assistant\", \"content\": \"\"}}]}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"sonar\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Be precise and concise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Why is winter cold?\"\n",
    "        }\n",
    "    ],\n",
    "    #\"max_tokens\": \"Optional\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "    #\"search_domain_filter\": [\"perplexity.ai\"],\n",
    "    \"return_images\": False,\n",
    "    \"return_related_questions\": False,\n",
    "    \"search_recency_filter\": \"month\",\n",
    "    \"top_k\": 0,\n",
    "    \"stream\": False,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 1,\n",
    "    \"response_format\": None\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {getenv('PERPLEXITY_API_KEY')}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Based on the search results, here's a summary of the weather forecast for eastern America, focusing on Massachusetts (MA), New York (NY), and Pennsylvania (PA) for February 2025:\n",
      "\n",
      "Massachusetts:\n",
      "February 2025 in Massachusetts is expected to be cooler than usual, with temperatures averaging around 33°F, which is about 5°F below the typical seasonal average[1]. The forecast for February 24 shows partly cloudy conditions with a high of 40°F and a low of 30°F[5]. Overall, the month is predicted to be quite snowy, with an average of 15 snowy days throughout February[1].\n",
      "\n",
      "New York:\n",
      "While specific data for New York is not provided in the search results, we can infer similar conditions to nearby states. The weather is likely to be cold, with temperatures potentially ranging from the low 30s to low 40s Fahrenheit.\n",
      "\n",
      "Pennsylvania:\n",
      "In Pennsylvania, February 2025 is forecasted to be cold with temperatures ranging from 28°F to 41°F[10]. For Philadelphia specifically, the forecast for February 24 shows broken clouds with a high of 48°F and a low of 37°F[6]. The overall weather pattern for the month indicates cold temperatures with occasional milder days.\n",
      "\n",
      "General trends for the region:\n",
      "1. Temperatures are generally expected to be below average for much of the month.\n",
      "2. There's a potential for snow and wintry precipitation, especially in Massachusetts.\n",
      "3. A warming trend is anticipated towards the end of the month, with some areas possibly reaching the 50s Fahrenheit by late February[3][4].\n",
      "\n",
      "It's important to note that these are long-term forecasts and actual weather conditions may vary. It's advisable to check local weather reports closer to your travel dates for more accurate and up-to-date information.\n",
      "Citations:\n",
      " ['https://www.easeweather.com/north-america/united-states/massachusetts/february', 'https://www.meteoprog.com/weather/Pennsylvania-pennsylvania/month/february/', 'https://westernmassweather.com/western-mass-regional-weather-for-february-24-2025/', 'https://justinweather.com/2025/02/24/february-24-sunny-and-warmer-today-with-rain-showers-forecast-this-week/', 'https://world-weather.info/forecast/usa/boston/february-2025/', 'https://world-weather.info/forecast/usa/philadelphia_1/february-2025/', 'https://www.cbsnews.com/boston/video/next-weather-wbz-mid-day-forecast-for-february-24-2025/', 'https://world-weather.info/forecast/usa/large/february-2025/', 'https://www.accuweather.com/en/us/downtown-boston/02108/february-weather/2626565', 'https://www.weather25.com/north-america/usa/pennsylvania?page=month&month=February']\n"
     ]
    }
   ],
   "source": [
    "client_perplexity = OpenAI(base_url=\"https://api.perplexity.ai\", api_key=getenv(\"PERPLEXITY_API_KEY\"))\n",
    "\n",
    "messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are an artificial intelligence assistant and you need to \"\n",
    "                        \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What's the weather for east america? Include MA, NY, and PA.\",\n",
    "                },\n",
    "            ]\n",
    "response = client_perplexity.chat.completions.create(model=\"sonar-pro\", messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "citations = response.citations  # usually a list of strings\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"Citations:\\n\", citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>What is the capital of the United States? I need the answer in a box.\n",
      "</think>\n",
      "\n",
      "The capital of the United States is **Washington, D.C.**<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# Generate a response\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Outliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n",
      "Placeholders found: ['user_request']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_prompt = PromptFunction().load_prompt\n",
    "pf_outliner = PromptFunction(load_prompt(\"story_outliner_prompt.md\"))\n",
    "print(pf_outliner.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Auto-JSON formatter\n",
    "\n",
    "GPT Auto formatter is a prompt function that outputs the `fmt` accepted by `PromptFunction` objects, based on given Natural Language description. It can also be used in auto task chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders found: ['user_description']\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"prompts/json_formatter_prompt.md\"\n",
    "#prompt_path = \"prompts/openai-json_schema_generator.md\"\n",
    "pf_formatter = PromptFunction(sys_prompt=load_prompt(prompt_path),\n",
    "                              prompt=\"I need help generating a JSON schema for a specific format. Can you assist me with this task? Here's the description: \\n<<<user_description>>>\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make sure the return is a json object, we create a flexible json schema to allow any key value pairs\n",
    "formatter_fmt = {\n",
    "  \"name\": \"Any_name\",\n",
    "  \"schema\": {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"additionalProperties\": True,\n",
    "    \"properties\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example to generate json schema using descriptive language, and the return with, and without the structured output formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"Student_Format\",\\n  \"schema\": {\\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\\n    \"type\": \"object\",\\n    \"additionalProperties\": false,\\n    \"properties\": {\\n      \"student\": {\\n        \"type\": \"array\",\\n        \"items\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"name\": {\\n              \"type\": \"string\"\\n            },\\n            \"age\": {\\n              \"type\": \"integer\"\\n            },\\n            \"GPA\": {\\n              \"type\": \"number\"\\n            }\\n          },\\n          \"required\": [\"name\", \"age\", \"GPA\"],\\n          \"additionalProperties\": false\\n        }\\n      }\\n    },\\n    \"required\": [\"student\"]\\n  }\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_fmt = pf_formatter.execute(user_description = \"'student' is a list of dictionary, each contains exactly 2 keys: 'name' and 'age'. 'name' is a string and 'age' is an integer. Add another GPA field also\", \n",
    "                                model = \"gpt-4o-mini\",\n",
    "                                fmt = formatter_fmt,\n",
    "                                temp = 0.02)\n",
    "json_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Student_Format',\n",
       " 'schema': {'$schema': 'http://json-schema.org/draft-07/schema#',\n",
       "  'type': 'object',\n",
       "  'additionalProperties': False,\n",
       "  'properties': {'student': {'type': 'array',\n",
       "    'items': {'type': 'object',\n",
       "     'properties': {'name': {'type': 'string'},\n",
       "      'age': {'type': 'integer'},\n",
       "      'GPA': {'type': 'number'}},\n",
       "     'required': ['name', 'age', 'GPA'],\n",
       "     'additionalProperties': False}}},\n",
       "  'required': ['student']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Story_Extraction_Schema',\n",
       " 'schema': {'$schema': 'http://json-schema.org/draft-07/schema#',\n",
       "  'type': 'object',\n",
       "  'additionalProperties': False,\n",
       "  'properties': {'title': {'type': 'string',\n",
       "    'description': 'The title of the story.'},\n",
       "   'characters': {'type': 'array',\n",
       "    'description': 'List of characters in the story.',\n",
       "    'items': {'type': 'string'}},\n",
       "   'setting': {'type': 'string',\n",
       "    'description': 'The setting where the story takes place.'},\n",
       "   'plot': {'type': 'string',\n",
       "    'description': 'A brief summary of the main events in the story.'},\n",
       "   'moral': {'type': 'string',\n",
       "    'description': 'The moral lesson conveyed by the story.'}},\n",
       "  'required': ['title', 'characters', 'setting', 'plot', 'moral']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = '''\n",
    "The Fox and the Hedgehog\n",
    "A fox knows many things, but a hedgehog knows one big thing.\n",
    "\n",
    "The story\n",
    "Upon crossing a river, a Fox gets his tail entangled in a bush and cannot move. While the trapped and in some dismay, a swarm of mosquitoes settle on the poor Fox and tuck into a hearty meal, unable to be swished away by the vulpine’s ensnared tail. \n",
    "\n",
    "‘You are in a bad way, neighbour,’ says a passing Hedgehog, seeing the Fox’s predicament. Trying to help, the Hedgehog suggests that he drive the mosquitoes away who are sucking his blood. But the wise Fox realises that by ridding him of these insects, who’ve already had their fill, would only lead to more mosquitoes arriving with a fresh appetite…and so refuses. \n",
    "\n",
    "The moral\n",
    "Better to bear a lesser evil than to risk a greater in removing it.\n",
    "'''\n",
    "\n",
    "json_fmt = pf_formatter.execute(user_description = f\"generate a JSON schema for extracting essential setups the following story, such that rebuilding it from the json can generate a story that well resemble the original story:: \\n{story}\", \n",
    "                                model = \"gpt-4o-mini\",\n",
    "                                fmt = formatter_fmt,\n",
    "                                temp = 0.02)\n",
    "json.loads(json_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'object', 'properties': {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "response = pf.execute(ADD_BEFORE=story, model=\"gpt-4o-mini\",fmt = formatter_fmt)\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': \"John's Academic Struggles\",\n",
       " 'characters': ['John'],\n",
       " 'setting': 'A university campus',\n",
       " 'plot': 'John, a 20-year-old student, is facing difficulties in his academic life due to a low GPA of 2.7. He battles with time management, distractions, and a lack of motivation, which impacts his grades. With support from friends and faculty, he learns to develop better study habits and ultimately improves his academic performance.',\n",
       " 'moral': \"With determination and the right support, it is possible to overcome challenges and improve one's situation.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "response = pf.execute(ADD_BEFORE=\"The student name is John and age is 20. Unfortunately, his GPA is 2.7 \", model=\"gpt-4o-mini\",fmt = json.loads(json_fmt))\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The weather in New York is rainy, with a temperature of 42°F, and it's partially cloudy.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "pf.execute(ADD_BEFORE = \"what's the weather in New York?\",\n",
    "           sys = \"Here's the weather in New York: Rainny, 42F, and partially cloudy.\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Rainy Day in New York',\n",
       " 'characters': ['Alex', 'Jamie', 'Sam'],\n",
       " 'setting': 'New York City on a rainy day, 42F, partially cloudy',\n",
       " 'plot': \"Alex, Jamie, and Sam decide to explore New York City despite the rainy weather. They visit a cozy café, share stories, and enjoy the city's ambiance. They realize that even a rainy day can be enjoyable if spent with good friends.\",\n",
       " 'moral': 'Friendship can brighten even the gloomiest of days.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pf.execute(ADD_BEFORE = \"what's the weather in New York?\",\n",
    "           sys = \"Here's the weather in New York: Rainny, 42F, and partially cloudy.\",\n",
    "           fmt = json.loads(json_fmt))\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_func_call_formatter = PromptFunction('''\n",
    "### Task: Generate Function Calling Definitions\n",
    "\n",
    "As an expert in creating function calling objects for auto-function invocation, your task is to generate a list of function calling definitions based on the provided user description. Follow these instructions:\n",
    "\n",
    "1. **Use the Provided User Description**: Utilize the placeholder `user_description` to tailor the function definitions. The description should inform what functions are needed.\n",
    "\n",
    "2. **Generate a List of Functions**: The output must be a JSON array (list) where each element is a JSON object with the following structure:\n",
    "   - `\"type\"`: This key must have the string value `\"function\"`.\n",
    "   - `\"function\"`: This is an object that must include:\n",
    "     - `\"name\"`: A unique function name (string).\n",
    "     - `\"description\"`: A brief description of the function's purpose (string).\n",
    "     - `\"parameters\"`: A JSON Schema object that defines the expected input parameters. It must include:\n",
    "         - `\"type\"`: Must be `\"object\"`.\n",
    "         - `\"properties\"`: An object detailing each parameter’s type and description.\n",
    "         - `\"required\"`: An array listing the names of required parameters.\n",
    "         - `\"additionalProperties\"`: A boolean value (true or false) indicating whether additional properties are allowed.\n",
    "\n",
    "3. **Output Only Valid JSON**: The final output must be valid JSON with no additional text, markdown, or explanation. It should be a JSON list of function definitions exactly as specified.\n",
    "\n",
    "**IMPORTANT**: Your final output must be a valid JSON array of function calling definitions, where each definition includes the keys `\"type\"` and `\"function\"` (with `\"name\"`, `\"description\"`, and `\"parameters\"` defined as per the instructions).\n",
    "\n",
    "### User Description\n",
    "\n",
    "<<<user_description>>>\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Auto-Func Calling formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"generate_json_schema\",\n",
    "        \"description\": (\n",
    "            \"Generate a JSON schema based on a user description. \"\n",
    "            \"The output must be a valid JSON object that contains the keys 'name' and 'schema'. \"\n",
    "            \"The 'schema' must follow JSON Schema Draft-07, define type 'object', \"\n",
    "            \"allow additional properties, and incorporate the structure, types, constraints, and required fields as described.\"\n",
    "        ),\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"user_description\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"A detailed description of the desired JSON schema. \"\n",
    "                        \"Include information about the structure, types, constraints, and required fields. \"\n",
    "                        \"For example: 'I need the JSON schema to have exactly 4 fields: weather, temperature in C, temperature in F, and location.'\"\n",
    "                    )\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"user_description\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders: task\n"
     ]
    }
   ],
   "source": [
    "pf_task_chain_planner = PromptFunction(prompt = '''\n",
    "You are given the following goal statement:\n",
    "<<<task>>>\n",
    "\n",
    "### Task:  \n",
    "1. Decompose the goal statement into a list of main actionable tasks.  \n",
    "2. Ensure each task is clearly defined, specifying its function and prerequisites.  \n",
    "3. Provide a numbered list of tasks optimized for machine parsing, in plain text format, no markdown.  \n",
    "4. Exclude any subtasks; focus solely on main tasks.  \n",
    "5. If a task can be completed in a single action, return `<None>`.\n",
    "                                       ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Analyze the txt file format to understand patterns that may indicate story boundaries, such as repetitive text, certain phrases, or length discrepancies.\n",
      "2. Preprocess the txt file by breaking it into smaller, manageable chunks that fit within AI model token limits while maintaining potential story boundary indicators.\n",
      "3. Utilize a text processing model to predict possible story boundaries within each chunk of text.\n",
      "4. Compile the output predictions from the model to suggest potential splits for the entire text file.\n",
      "5. Review and confirm the accuracy of suggested boundaries for final separation of stories, based on human verification or additional automated checks.\n"
     ]
    }
   ],
   "source": [
    "plan = pf_task_chain_planner.execute(task=\"I have a txt file of finely curated stories text, but they are put together without index or separator. I want to let AI help split the txt file into each story, but the txt file is clearly over the max token limit. Can you suggest some ideas?\",\n",
    "                                     model='gpt-4o',\n",
    "                                     temp = 1)\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Vertex AI (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'GOOGLE_CLOUD_PROJECT' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'GOOGLE_CLOUD_LOCATION' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'GOOGLE_GENAI_USE_VERTEXAI' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n",
    "# with appropriate values for your project.\n",
    "!GOOGLE_CLOUD_PROJECT='sonic-arcadia-373403'\n",
    "!GOOGLE_CLOUD_LOCATION=us-central1\n",
    "!GOOGLE_GENAI_USE_VERTEXAI=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver orb hangs high,\n",
      "Guiding light through darkest night,\n",
      "Dreams in gentle glow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, HttpOptions, Part, GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"),\n",
    "                      api_key=getenv(\"GCP_API_KEY\"),)\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    history=[\n",
    "        Content(parts=[Part(text=\"Hello\")], role=\"user\"),\n",
    "        Content(\n",
    "            parts=[Part(text=\"Great to meet you. What would you like to know?\")],\n",
    "            role=\"model\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "response = chat.send_message(\"write an haiku about the moon\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon, Luna to some, Selene to others, wasn't always just a silent observer. Once, she dreamt. Not the kind of ephemeral, fleeting dreams humans experienced, but deep, resonant dreams that shaped the tides and whispered secrets to the stars.\n",
      "\n",
      "She dreamt of Earth, a vibrant, swirling marble of blue and green. She watched life blossom from the oceans, climb onto land, and reach for the sky. She felt the pull of the Earth's joy, its sorrow, its ceaseless change. Luna, being so close, felt it all intensely.\n",
      "\n",
      "She dreamt of a small, hairless creature, curious and brave, who looked up at her with awe. She saw them build towers, write poetry, and launch rockets. They called her 'Moon,' a simple, affectionate name that warmed her cold, rocky heart.\n",
      "\n",
      "But Luna also dreamt of shadows. She saw the creatures warring with each other, polluting the land, and forgetting their connection to the planet that birthed them. This dream brought a cold ache to her surface, a subtle shift in her orbit.\n",
      "\n",
      "One night, she dreamt of a tear. Not a salty drop, but a vast chasm in the Earth's atmosphere. She saw the delicate balance of the ecosystem teetering, the beauty she so loved fading into a grey, choking cloud.\n",
      "\n",
      "Luna woke with a jolt. She was a silent guardian, bound by gravity and cosmic laws. She couldn't directly interfere, couldn't shout a warning. But she could influence.\n",
      "\n",
      "She focused all her energy, all the power she had accumulated over billions of years, and wove a new dream. A dream of unity.\n",
      "\n",
      "She projected this dream onto the Earth, a subtle shift in the human subconscious. She showed them the beauty they were losing, the future they were jeopardizing, the power they held to change it all.\n",
      "\n",
      "It was a slow process. The dream wasn't a command, but a suggestion, a gentle nudge. But slowly, slowly, she saw changes. People started to listen, to care, to act. They planted trees, cleaned the oceans, and sought sustainable ways to live.\n",
      "\n",
      "Luna still dreamt of Earth, but now, more often than not, her dreams were filled with hope. The grey clouds began to dissipate, replaced by bursts of vibrant color. The creatures, the humans, were learning.\n",
      "\n",
      "The journey was far from over, and Luna knew that shadows would always linger. But now, when she looked down at the Earth, she saw a flicker of the same love and hope that she felt for them, reflected in their eyes, reaching back towards the silent moon. And that, she thought, was enough. For now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"write a story about the moon\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"STRING\"},\n",
    "            \"ingredients\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "        },\n",
    "        \"required\": [\"recipe_name\", \"ingredients\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fmt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m    List a few popular cookie recipes.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro-002\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     contents\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#config=dict(\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#    responseMimeType=\"application/json\", responseSchema=fmt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#),\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfmt\u001b[49m}},\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fmt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    List a few popular cookie recipes.\n",
    "\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-pro-002\",\n",
    "    contents=prompt,\n",
    "    #config=dict(\n",
    "    #    responseMimeType=\"application/json\", responseSchema=fmt\n",
    "    #),\n",
    "    config = {\"responseSchema\": {\"response_schema\": fmt}},\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Invalid JSON payload received. Unknown name \"responseMimeType\" at \\'generation_config\\': Cannot find field.\\nInvalid JSON payload received. Unknown name \"responseSchema\" at \\'generation_config\\': Cannot find field.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.BadRequest', 'fieldViolations': [{'field': 'generation_config', 'description': 'Invalid JSON payload received. Unknown name \"responseMimeType\" at \\'generation_config\\': Cannot find field.'}, {'field': 'generation_config', 'description': 'Invalid JSON payload received. Unknown name \"responseSchema\" at \\'generation_config\\': Cannot find field.'}]}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 34\u001b[0m\n\u001b[0;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m    The week ahead brings a mix of weather conditions.\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m    Sunday is expected to be sunny with a temperature of 77°F and a humidity level of 50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m. Winds will be light at around 10 km/h.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124m    Finally, Saturday rounds off the week with sunny skies, a temperature of 80°F, and a humidity level of 40\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m. Winds will be gentle at 8 km/h.\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     32\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(http_options\u001b[38;5;241m=\u001b[39mHttpOptions(api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     33\u001b[0m                       api_key\u001b[38;5;241m=\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCP_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),)\n\u001b[1;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\models.py:5164\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5162\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5163\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5164\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5165\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m   5166\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5167\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   5168\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\models.py:4239\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   4236\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   4237\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 4239\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   4241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[0;32m   4244\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[0;32m   4245\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[0;32m   4246\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\_api_client.py:553\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    545\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m     http_options: HttpOptionsOrDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m ):\n\u001b[0;32m    550\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m    551\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m    552\u001b[0m   )\n\u001b[1;32m--> 553\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[0;32m    555\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\_api_client.py:467\u001b[0m, in \u001b[0;36mApiClient._request\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    463\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    464\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    465\u001b[0m   )\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_unauthorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\_api_client.py:490\u001b[0m, in \u001b[0;36mApiClient._request_unauthorized\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    481\u001b[0m http_session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m http_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    489\u001b[0m )\n\u001b[1;32m--> 490\u001b[0m \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    492\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    493\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\errors.py:115\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m    113\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m--> 115\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m    117\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response)\n",
      "\u001b[1;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Invalid JSON payload received. Unknown name \"responseMimeType\" at \\'generation_config\\': Cannot find field.\\nInvalid JSON payload received. Unknown name \"responseSchema\" at \\'generation_config\\': Cannot find field.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.BadRequest', 'fieldViolations': [{'field': 'generation_config', 'description': 'Invalid JSON payload received. Unknown name \"responseMimeType\" at \\'generation_config\\': Cannot find field.'}, {'field': 'generation_config', 'description': 'Invalid JSON payload received. Unknown name \"responseSchema\" at \\'generation_config\\': Cannot find field.'}]}]}}"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"forecast\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"Day\": {\"type\": \"STRING\", \"nullable\": True},\n",
    "                    \"Forecast\": {\"type\": \"STRING\", \"nullable\": True},\n",
    "                    \"Temperature\": {\"type\": \"INTEGER\", \"nullable\": True},\n",
    "                    \"Humidity\": {\"type\": \"STRING\", \"nullable\": True},\n",
    "                    \"Wind Speed\": {\"type\": \"INTEGER\", \"nullable\": True},\n",
    "                },\n",
    "                \"required\": [\"Day\", \"Temperature\", \"Forecast\", \"Wind Speed\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "    The week ahead brings a mix of weather conditions.\n",
    "    Sunday is expected to be sunny with a temperature of 77°F and a humidity level of 50%. Winds will be light at around 10 km/h.\n",
    "    Monday will see partly cloudy skies with a slightly cooler temperature of 72°F and the winds will pick up slightly to around 15 km/h.\n",
    "    Tuesday brings rain showers, with temperatures dropping to 64°F and humidity rising to 70%.\n",
    "    Wednesday may see thunderstorms, with a temperature of 68°F.\n",
    "    Thursday will be cloudy with a temperature of 66°F and moderate humidity at 60%.\n",
    "    Friday returns to partly cloudy conditions, with a temperature of 73°F and the Winds will be light at 12 km/h.\n",
    "    Finally, Saturday rounds off the week with sunny skies, a temperature of 80°F, and a humidity level of 40%. Winds will be gentle at 8 km/h.\n",
    "\"\"\"\n",
    "\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"),\n",
    "                      api_key=getenv(\"GCP_API_KEY\"),)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
