{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from POP import PromptFunction\n",
    "#from POP.utils import load_prompt, get_text_snapshot\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"a021e45e-0446-49d0-855a-a15021a17964\", \"model\": \"sonar\", \"created\": 1740437547, \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 130, \"total_tokens\": 140}, \"citations\": [\"https://www.dw.com/en/earth-is-warming-so-why-is-it-so-cold/a-71661055\", \"https://www.climate.gov/news-features/blogs/polar-vortex/cold-air-its-probably-not-polar-vortex\", \"https://www.nbcwashington.com/weather/science-4-everyone-how-do-bugs-survive-during-the-cold-winter-months/3850831/\"], \"object\": \"chat.completion\", \"choices\": [{\"index\": 0, \"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Winter cold is primarily due to Earth's tilt away from the Sun during this season, reducing the amount of solar energy received by the Northern Hemisphere. Additionally, cold air masses from polar regions can move towards lower latitudes, bringing frigid temperatures. In recent years, phenomena like the **polar vortex** have been linked to extreme cold snaps. The polar vortex is a band of winds that usually keeps cold air contained over the poles. However, when it weakens or becomes more unstable due to climate change, it can allow cold air to escape and move towards more temperate regions, leading to sudden and intense cold snaps[1][2].\"}, \"delta\": {\"role\": \"assistant\", \"content\": \"\"}}]}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"sonar\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Be precise and concise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Why is winter cold?\"\n",
    "        }\n",
    "    ],\n",
    "    #\"max_tokens\": \"Optional\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "    #\"search_domain_filter\": [\"perplexity.ai\"],\n",
    "    \"return_images\": False,\n",
    "    \"return_related_questions\": False,\n",
    "    \"search_recency_filter\": \"month\",\n",
    "    \"top_k\": 0,\n",
    "    \"stream\": False,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 1,\n",
    "    \"response_format\": None\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {getenv('PERPLEXITY_API_KEY')}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Based on the search results, here's a summary of the weather forecast for eastern America, focusing on Massachusetts (MA), New York (NY), and Pennsylvania (PA) for February 2025:\n",
      "\n",
      "Massachusetts:\n",
      "February 2025 in Massachusetts is expected to be cooler than usual, with temperatures averaging around 33°F, which is about 5°F below the typical seasonal average[1]. The forecast for February 24 shows partly cloudy conditions with a high of 40°F and a low of 30°F[5]. Overall, the month is predicted to be quite snowy, with an average of 15 snowy days throughout February[1].\n",
      "\n",
      "New York:\n",
      "While specific data for New York is not provided in the search results, we can infer similar conditions to nearby states. The weather is likely to be cold, with temperatures potentially ranging from the low 30s to low 40s Fahrenheit.\n",
      "\n",
      "Pennsylvania:\n",
      "In Pennsylvania, February 2025 is forecasted to be cold with temperatures ranging from 28°F to 41°F[10]. For Philadelphia specifically, the forecast for February 24 shows broken clouds with a high of 48°F and a low of 37°F[6]. The overall weather pattern for the month indicates cold temperatures with occasional milder days.\n",
      "\n",
      "General trends for the region:\n",
      "1. Temperatures are generally expected to be below average for much of the month.\n",
      "2. There's a potential for snow and wintry precipitation, especially in Massachusetts.\n",
      "3. A warming trend is anticipated towards the end of the month, with some areas possibly reaching the 50s Fahrenheit by late February[3][4].\n",
      "\n",
      "It's important to note that these are long-term forecasts and actual weather conditions may vary. It's advisable to check local weather reports closer to your travel dates for more accurate and up-to-date information.\n",
      "Citations:\n",
      " ['https://www.easeweather.com/north-america/united-states/massachusetts/february', 'https://www.meteoprog.com/weather/Pennsylvania-pennsylvania/month/february/', 'https://westernmassweather.com/western-mass-regional-weather-for-february-24-2025/', 'https://justinweather.com/2025/02/24/february-24-sunny-and-warmer-today-with-rain-showers-forecast-this-week/', 'https://world-weather.info/forecast/usa/boston/february-2025/', 'https://world-weather.info/forecast/usa/philadelphia_1/february-2025/', 'https://www.cbsnews.com/boston/video/next-weather-wbz-mid-day-forecast-for-february-24-2025/', 'https://world-weather.info/forecast/usa/large/february-2025/', 'https://www.accuweather.com/en/us/downtown-boston/02108/february-weather/2626565', 'https://www.weather25.com/north-america/usa/pennsylvania?page=month&month=February']\n"
     ]
    }
   ],
   "source": [
    "client_perplexity = OpenAI(base_url=\"https://api.perplexity.ai\", api_key=getenv(\"PERPLEXITY_API_KEY\"))\n",
    "\n",
    "messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are an artificial intelligence assistant and you need to \"\n",
    "                        \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What's the weather for east america? Include MA, NY, and PA.\",\n",
    "                },\n",
    "            ]\n",
    "response = client_perplexity.chat.completions.create(model=\"sonar-pro\", messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "citations = response.citations  # usually a list of strings\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"Citations:\\n\", citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sgt17\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>What is the capital of the United States? I need the answer in a box.\n",
      "</think>\n",
      "\n",
      "The capital of the United States is **Washington, D.C.**<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# Generate a response\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Outliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n",
      "Placeholders found: ['user_request']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_prompt = PromptFunction().load_prompt\n",
    "pf_outliner = PromptFunction(load_prompt(\"story_outliner_prompt.md\"))\n",
    "print(pf_outliner.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Auto-JSON formatter\n",
    "\n",
    "GPT Auto formatter is a prompt function that outputs the `fmt` accepted by `PromptFunction` objects, based on given Natural Language description. It can also be used in auto task chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders found: ['user_description']\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"prompts/json_formatter_prompt.md\"\n",
    "#prompt_path = \"prompts/openai-json_schema_generator.md\"\n",
    "pf_formatter = PromptFunction(sys_prompt=load_prompt(prompt_path),\n",
    "                              prompt=\"I need help generating a JSON schema for a specific format. Can you assist me with this task? Here's the description: \\n<<<user_description>>>\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make sure the return is a json object, we create a flexible json schema to allow any key value pairs\n",
    "formatter_fmt = {\n",
    "  \"name\": \"Any_name\",\n",
    "  \"schema\": {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"additionalProperties\": True,\n",
    "    \"properties\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example to generate json schema using descriptive language, and the return with, and without the structured output formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"Student_Format\",\\n  \"schema\": {\\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\\n    \"type\": \"object\",\\n    \"additionalProperties\": false,\\n    \"properties\": {\\n      \"student\": {\\n        \"type\": \"array\",\\n        \"items\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"name\": {\\n              \"type\": \"string\"\\n            },\\n            \"age\": {\\n              \"type\": \"integer\"\\n            },\\n            \"GPA\": {\\n              \"type\": \"number\"\\n            }\\n          },\\n          \"required\": [\"name\", \"age\", \"GPA\"],\\n          \"additionalProperties\": false\\n        }\\n      }\\n    },\\n    \"required\": [\"student\"]\\n  }\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_fmt = pf_formatter.execute(user_description = \"'student' is a list of dictionary, each contains exactly 2 keys: 'name' and 'age'. 'name' is a string and 'age' is an integer. Add another GPA field also\", \n",
    "                                model = \"gpt-4o-mini\",\n",
    "                                fmt = formatter_fmt,\n",
    "                                temp = 0.02)\n",
    "json_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Student_Format',\n",
       " 'schema': {'$schema': 'http://json-schema.org/draft-07/schema#',\n",
       "  'type': 'object',\n",
       "  'additionalProperties': False,\n",
       "  'properties': {'student': {'type': 'array',\n",
       "    'items': {'type': 'object',\n",
       "     'properties': {'name': {'type': 'string'},\n",
       "      'age': {'type': 'integer'},\n",
       "      'GPA': {'type': 'number'}},\n",
       "     'required': ['name', 'age', 'GPA'],\n",
       "     'additionalProperties': False}}},\n",
       "  'required': ['student']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Story_Extraction_Schema',\n",
       " 'schema': {'$schema': 'http://json-schema.org/draft-07/schema#',\n",
       "  'type': 'object',\n",
       "  'additionalProperties': False,\n",
       "  'properties': {'title': {'type': 'string',\n",
       "    'description': 'The title of the story.'},\n",
       "   'characters': {'type': 'array',\n",
       "    'description': 'List of characters in the story.',\n",
       "    'items': {'type': 'string'}},\n",
       "   'setting': {'type': 'string',\n",
       "    'description': 'The setting where the story takes place.'},\n",
       "   'plot': {'type': 'string',\n",
       "    'description': 'A brief summary of the main events in the story.'},\n",
       "   'moral': {'type': 'string',\n",
       "    'description': 'The moral lesson conveyed by the story.'}},\n",
       "  'required': ['title', 'characters', 'setting', 'plot', 'moral']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = '''\n",
    "The Fox and the Hedgehog\n",
    "A fox knows many things, but a hedgehog knows one big thing.\n",
    "\n",
    "The story\n",
    "Upon crossing a river, a Fox gets his tail entangled in a bush and cannot move. While the trapped and in some dismay, a swarm of mosquitoes settle on the poor Fox and tuck into a hearty meal, unable to be swished away by the vulpine’s ensnared tail. \n",
    "\n",
    "‘You are in a bad way, neighbour,’ says a passing Hedgehog, seeing the Fox’s predicament. Trying to help, the Hedgehog suggests that he drive the mosquitoes away who are sucking his blood. But the wise Fox realises that by ridding him of these insects, who’ve already had their fill, would only lead to more mosquitoes arriving with a fresh appetite…and so refuses. \n",
    "\n",
    "The moral\n",
    "Better to bear a lesser evil than to risk a greater in removing it.\n",
    "'''\n",
    "\n",
    "json_fmt = pf_formatter.execute(user_description = f\"generate a JSON schema for extracting essential setups the following story, such that rebuilding it from the json can generate a story that well resemble the original story:: \\n{story}\", \n",
    "                                model = \"gpt-4o-mini\",\n",
    "                                fmt = formatter_fmt,\n",
    "                                temp = 0.02)\n",
    "json.loads(json_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'object', 'properties': {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "response = pf.execute(ADD_BEFORE=story, model=\"gpt-4o-mini\",fmt = formatter_fmt)\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': \"John's Academic Struggles\",\n",
       " 'characters': ['John'],\n",
       " 'setting': 'A university campus',\n",
       " 'plot': 'John, a 20-year-old student, is facing difficulties in his academic life due to a low GPA of 2.7. He battles with time management, distractions, and a lack of motivation, which impacts his grades. With support from friends and faculty, he learns to develop better study habits and ultimately improves his academic performance.',\n",
       " 'moral': \"With determination and the right support, it is possible to overcome challenges and improve one's situation.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "response = pf.execute(ADD_BEFORE=\"The student name is John and age is 20. Unfortunately, his GPA is 2.7 \", model=\"gpt-4o-mini\",fmt = json.loads(json_fmt))\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt or system prompt provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The weather in New York is rainy, with a temperature of 42°F, and it's partially cloudy.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = PromptFunction('')\n",
    "pf.execute(ADD_BEFORE = \"what's the weather in New York?\",\n",
    "           sys = \"Here's the weather in New York: Rainny, 42F, and partially cloudy.\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Rainy Day in New York',\n",
       " 'characters': ['Alex', 'Jamie', 'Sam'],\n",
       " 'setting': 'New York City on a rainy day, 42F, partially cloudy',\n",
       " 'plot': \"Alex, Jamie, and Sam decide to explore New York City despite the rainy weather. They visit a cozy café, share stories, and enjoy the city's ambiance. They realize that even a rainy day can be enjoyable if spent with good friends.\",\n",
       " 'moral': 'Friendship can brighten even the gloomiest of days.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pf.execute(ADD_BEFORE = \"what's the weather in New York?\",\n",
    "           sys = \"Here's the weather in New York: Rainny, 42F, and partially cloudy.\",\n",
    "           fmt = json.loads(json_fmt))\n",
    "story_setup = json.loads(response)\n",
    "story_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_func_call_formatter = PromptFunction('''\n",
    "### Task: Generate Function Calling Definitions\n",
    "\n",
    "As an expert in creating function calling objects for auto-function invocation, your task is to generate a list of function calling definitions based on the provided user description. Follow these instructions:\n",
    "\n",
    "1. **Use the Provided User Description**: Utilize the placeholder `user_description` to tailor the function definitions. The description should inform what functions are needed.\n",
    "\n",
    "2. **Generate a List of Functions**: The output must be a JSON array (list) where each element is a JSON object with the following structure:\n",
    "   - `\"type\"`: This key must have the string value `\"function\"`.\n",
    "   - `\"function\"`: This is an object that must include:\n",
    "     - `\"name\"`: A unique function name (string).\n",
    "     - `\"description\"`: A brief description of the function's purpose (string).\n",
    "     - `\"parameters\"`: A JSON Schema object that defines the expected input parameters. It must include:\n",
    "         - `\"type\"`: Must be `\"object\"`.\n",
    "         - `\"properties\"`: An object detailing each parameter’s type and description.\n",
    "         - `\"required\"`: An array listing the names of required parameters.\n",
    "         - `\"additionalProperties\"`: A boolean value (true or false) indicating whether additional properties are allowed.\n",
    "\n",
    "3. **Output Only Valid JSON**: The final output must be valid JSON with no additional text, markdown, or explanation. It should be a JSON list of function definitions exactly as specified.\n",
    "\n",
    "**IMPORTANT**: Your final output must be a valid JSON array of function calling definitions, where each definition includes the keys `\"type\"` and `\"function\"` (with `\"name\"`, `\"description\"`, and `\"parameters\"` defined as per the instructions).\n",
    "\n",
    "### User Description\n",
    "\n",
    "<<<user_description>>>\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Auto-Func Calling formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"generate_json_schema\",\n",
    "        \"description\": (\n",
    "            \"Generate a JSON schema based on a user description. \"\n",
    "            \"The output must be a valid JSON object that contains the keys 'name' and 'schema'. \"\n",
    "            \"The 'schema' must follow JSON Schema Draft-07, define type 'object', \"\n",
    "            \"allow additional properties, and incorporate the structure, types, constraints, and required fields as described.\"\n",
    "        ),\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"user_description\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"A detailed description of the desired JSON schema. \"\n",
    "                        \"Include information about the structure, types, constraints, and required fields. \"\n",
    "                        \"For example: 'I need the JSON schema to have exactly 4 fields: weather, temperature in C, temperature in F, and location.'\"\n",
    "                    )\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"user_description\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders: task\n"
     ]
    }
   ],
   "source": [
    "pf_task_chain_planner = PromptFunction(prompt = '''\n",
    "You are given the following goal statement:\n",
    "<<<task>>>\n",
    "\n",
    "### Task:  \n",
    "1. Decompose the goal statement into a list of main actionable tasks.  \n",
    "2. Ensure each task is clearly defined, specifying its function and prerequisites.  \n",
    "3. Provide a numbered list of tasks optimized for machine parsing, in plain text format, no markdown.  \n",
    "4. Exclude any subtasks; focus solely on main tasks.  \n",
    "5. If a task can be completed in a single action, return `<None>`.\n",
    "                                       ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Analyze the txt file format to understand patterns that may indicate story boundaries, such as repetitive text, certain phrases, or length discrepancies.\n",
      "2. Preprocess the txt file by breaking it into smaller, manageable chunks that fit within AI model token limits while maintaining potential story boundary indicators.\n",
      "3. Utilize a text processing model to predict possible story boundaries within each chunk of text.\n",
      "4. Compile the output predictions from the model to suggest potential splits for the entire text file.\n",
      "5. Review and confirm the accuracy of suggested boundaries for final separation of stories, based on human verification or additional automated checks.\n"
     ]
    }
   ],
   "source": [
    "plan = pf_task_chain_planner.execute(task=\"I have a txt file of finely curated stories text, but they are put together without index or separator. I want to let AI help split the txt file into each story, but the txt file is clearly over the max token limit. Can you suggest some ideas?\",\n",
    "                                     model='gpt-4o',\n",
    "                                     temp = 1)\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Vertex AI (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'GOOGLE_CLOUD_PROJECT' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'GOOGLE_CLOUD_LOCATION' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'GOOGLE_GENAI_USE_VERTEXAI' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n",
    "# with appropriate values for your project.\n",
    "!GOOGLE_CLOUD_PROJECT='sonic-arcadia-373403'\n",
    "!GOOGLE_CLOUD_LOCATION=us-central1\n",
    "!GOOGLE_GENAI_USE_VERTEXAI=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver orb ascends,\n",
      "Guiding light in velvet skies,\n",
      "Dreams in shadows dance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, HttpOptions, Part\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"),\n",
    "                      api_key=getenv(\"GCP_API_KEY\"),)\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    history=[\n",
    "        Content(parts=[Part(text=\"Hello\")], role=\"user\"),\n",
    "        Content(\n",
    "            parts=[Part(text=\"Great to meet you. What would you like to know?\")],\n",
    "            role=\"model\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "response = chat.send_message(\"write an haiku about the moon\")\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# Okay, here's a story for you:\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
